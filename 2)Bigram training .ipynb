{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c95d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEURAL NETWORK APPROACH TO BIGRAMS\n",
    "\n",
    "#we train the network to use probabilities to find the character that'll follow a character. \n",
    "#we train using loss function - max likelihood function\n",
    "#training set : all the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69847865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83aca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e1077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) \n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} \n",
    "\n",
    "stoi['.'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10a090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating training set\n",
    "xs,ys=[],[] #xs denotes 1st character. ys denote the char that follows\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs=torch.tensor(xs)\n",
    "ys=torch.tensor(ys)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b48e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one_hot vector to feed an integer type input into the neurel net.\n",
    "#ie. 13 will be a vector of n dim with all 0s except 13th position\n",
    "#torch has inbuilt func to create onehot vector\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xenc=F.one_hot(xs,num_classes=27).float() \n",
    "xenc\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2e9e12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9334,  0.7547, -0.4723,  ...,  0.0846, -1.3133, -0.3960],\n",
       "        [ 2.3214,  0.8544,  0.8441,  ..., -0.4428, -1.2350, -0.8293],\n",
       "        [ 0.6428,  1.6423, -0.6300,  ..., -0.6764,  1.1980,  0.6434],\n",
       "        ...,\n",
       "        [-0.4630,  0.8062, -0.8079,  ...,  0.8174, -0.4841,  1.0849],\n",
       "        [-1.3634, -0.5307, -0.1548,  ...,  0.7487, -0.0275,  1.3942],\n",
       "        [ 0.9246, -0.9739,  1.1087,  ..., -0.0166, -0.9982, -0.0034]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W=torch.randn((27,1))#1 neuron\n",
    "\n",
    "W=torch.randn((27,27), requires_grad=True) # 27 neurons\n",
    "xenc @ W #matrix multiplication in torch is done by @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a383f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3932,  2.1270,  0.6236,  ...,  1.0883,  0.2689,  0.6730],\n",
       "        [10.1904,  2.3501,  2.3260,  ...,  0.6422,  0.2908,  0.4363],\n",
       "        [ 1.9018,  5.1672,  0.5326,  ...,  0.5085,  3.3135,  1.9029],\n",
       "        ...,\n",
       "        [ 0.6294,  2.2395,  0.4458,  ...,  2.2647,  0.6163,  2.9592],\n",
       "        [ 0.2558,  0.5882,  0.8566,  ...,  2.1142,  0.9728,  4.0319],\n",
       "        [ 2.5208,  0.3776,  3.0305,  ...,  0.9835,  0.3685,  0.9966]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want the layer to output log counts (We cant get prob or counts) #Efficiency: probs would be very small.. log\n",
    "# ???????? Prob using softmax activation func\n",
    "\n",
    "xenc @ W.exp() #negative numbers are made positive(0-1) and positive numbers are made more positive(>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5da4b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0135, 0.0731, 0.0214,  ..., 0.0374, 0.0092, 0.0231],\n",
       "        [0.1738, 0.0401, 0.0397,  ..., 0.0110, 0.0050, 0.0074],\n",
       "        [0.0484, 0.1314, 0.0135,  ..., 0.0129, 0.0842, 0.0484],\n",
       "        ...,\n",
       "        [0.0130, 0.0464, 0.0092,  ..., 0.0469, 0.0128, 0.0613],\n",
       "        [0.0068, 0.0157, 0.0229,  ..., 0.0565, 0.0260, 0.1078],\n",
       "        [0.0428, 0.0064, 0.0514,  ..., 0.0167, 0.0063, 0.0169]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=xenc @ W #log-counts\n",
    "counts=logits.exp() #equivalent to N in the table previously\n",
    "probs=counts/counts.sum(1,keepdims=True) #this is softmax activation func\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fec9370f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0135, 0.0731, 0.0214, 0.0333, 0.0326, 0.0129, 0.0223, 0.2328, 0.0579,\n",
       "        0.0605, 0.0415, 0.0390, 0.0110, 0.0152, 0.0346, 0.0362, 0.0135, 0.0304,\n",
       "        0.0180, 0.0158, 0.0073, 0.0257, 0.0092, 0.0725, 0.0374, 0.0092, 0.0231],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0] \n",
    "#this gives the array of probs of each 27 character following '.'(0th element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae692e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have the output. but it is not optimised. we need to reduce the loss function to get the perfect weights of the neurons\n",
    "\n",
    "# now comes the use of micrograd.\n",
    "\n",
    "#Softmax doesnt require bias\n",
    "\n",
    "# forward pass #activation(xw)\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "\n",
    "loss = -probs[torch.arange(228146), ys].log().mean() #avg negative log likelihood loss func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "060800a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7154, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79ac4447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0129, 0.0599, 0.0434,  ..., 0.0613, 0.0565, 0.0428],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probs[0,5] #1st bigram .e #ideally should be one\n",
    "#probs[1,13]#2nd bigram ys shld be m\n",
    "#instead of checking each bigram individually we do the following\n",
    "probs[torch.arange(228146),ys] #ideally shld be a array of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "127015ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass\n",
    "W.grad = None # set to zero the gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "731a2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508a398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
